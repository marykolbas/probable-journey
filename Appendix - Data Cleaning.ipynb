{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2346fb9",
   "metadata": {},
   "source": [
    "## Data Cleaning (Appendix, Draft)\n",
    "\n",
    "Project FeederWatch is a citizen-science-based data source supported by the Cornell Lab of Ornithology, which collects observations of bird species at backyard feeders and habitats all over the world in an annual November-April survey.\n",
    "\n",
    "Our raw file comes from the [Project FeederWatch](https://feederwatch.org/explore/raw-dataset-requests/) 2021 New York checklist data and site description data. This file is extremely large and has sightings from about November 2020 to April 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68269300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688d363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sql\n",
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "%sql duckdb:///:memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69611532",
   "metadata": {},
   "source": [
    "## Joining Original Dataset (not zero-filled) with Sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f24ce9",
   "metadata": {},
   "source": [
    "Our first dataset joins the original dataset of 2021 Season Feederwatch Observations with the sites dataset with location information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e207df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in original dataset from Feederwatch for observations\n",
    "total_df = pd.read_csv(\"PFW_2021_public.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ae56c",
   "metadata": {},
   "source": [
    "We chose to drop several columns in the observations dataset (`total_df`) such that only `loc_id`, `subnational1_code`, and `species_code` are present. We also chose to eliminate most columns from the sites dataset (`sites_df`) so that only `loc_id` and `housing_density` remain, allowing for an efficient data join. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e95db9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>subnational1_code</th>\n",
       "      <th>species_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L12782033</td>\n",
       "      <td>CA-ON</td>\n",
       "      <td>amtspa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L12782033</td>\n",
       "      <td>CA-ON</td>\n",
       "      <td>blujay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L12782033</td>\n",
       "      <td>CA-ON</td>\n",
       "      <td>bkcchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L12755941</td>\n",
       "      <td>CA-SK</td>\n",
       "      <td>dowwoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L12755941</td>\n",
       "      <td>CA-SK</td>\n",
       "      <td>whbnut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897100</th>\n",
       "      <td>L2404002</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>daejun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897101</th>\n",
       "      <td>L2404002</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>amerob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897102</th>\n",
       "      <td>L2404002</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>houfin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897103</th>\n",
       "      <td>L2404002</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>norcar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897104</th>\n",
       "      <td>L2404002</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>rebnut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2897105 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            loc_id subnational1_code species_code\n",
       "0        L12782033             CA-ON       amtspa\n",
       "1        L12782033             CA-ON       blujay\n",
       "2        L12782033             CA-ON       bkcchi\n",
       "3        L12755941             CA-SK       dowwoo\n",
       "4        L12755941             CA-SK       whbnut\n",
       "...            ...               ...          ...\n",
       "2897100   L2404002             US-NY       daejun\n",
       "2897101   L2404002             US-NY       amerob\n",
       "2897102   L2404002             US-NY       houfin\n",
       "2897103   L2404002             US-NY       norcar\n",
       "2897104   L2404002             US-NY       rebnut\n",
       "\n",
       "[2897105 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.columns\n",
    "total_df = total_df.drop(['latitude', 'longitude',\n",
    "       'entry_technique', 'sub_id', 'obs_id', 'Month', 'Day', 'Year',\n",
    "       'PROJ_PERIOD_ID', 'how_many', 'valid', 'reviewed',\n",
    "       'day1_am', 'day1_pm', 'day2_am', 'day2_pm', 'effort_hrs_atleast',\n",
    "       'snow_dep_atleast', 'Data_Entry_Method'], axis = 1)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2696b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>housing_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    loc_id  housing_density\n",
       "0  L100016              2.0\n",
       "1  L100016              2.0\n",
       "2  L100016              2.0\n",
       "3  L100016              2.0\n",
       "4  L100016              2.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in original dataset from Feederwatch for locations\n",
    "sites_df = pd.read_csv(\"PFW_count_site_data_public_2021.csv\")\n",
    "\n",
    "sites_df = sites_df.drop(['proj_period_id', 'yard_type_pavement', 'yard_type_garden',\n",
    "       'yard_type_landsca', 'yard_type_woods', 'yard_type_desert',\n",
    "       'hab_dcid_woods', 'hab_evgr_woods', 'hab_mixed_woods', 'hab_orchard',\n",
    "       'hab_park', 'hab_water_fresh', 'hab_water_salt', 'hab_residential',\n",
    "       'hab_industrial', 'hab_agricultural', 'hab_desert_scrub',\n",
    "       'hab_young_woods', 'hab_swamp', 'hab_marsh', 'evgr_trees_atleast',\n",
    "       'evgr_shrbs_atleast', 'dcid_trees_atleast', 'dcid_shrbs_atleast',\n",
    "       'fru_trees_atleast', 'cacti_atleast', 'brsh_piles_atleast',\n",
    "       'water_srcs_atleast', 'bird_baths_atleast', 'nearby_feeders',\n",
    "       'squirrels', 'cats', 'dogs', 'humans',\n",
    "       'fed_yr_round', 'fed_in_jan', 'fed_in_feb', 'fed_in_mar', 'fed_in_apr',\n",
    "       'fed_in_may', 'fed_in_jun', 'fed_in_jul', 'fed_in_aug', 'fed_in_sep',\n",
    "       'fed_in_oct', 'fed_in_nov', 'fed_in_dec', 'numfeeders_suet',\n",
    "       'numfeeders_ground', 'numfeeders_hanging', 'numfeeders_platfrm',\n",
    "       'numfeeders_humming', 'numfeeders_water', 'numfeeders_thistle',\n",
    "       'numfeeders_fruit', 'numfeeders_hopper', 'numfeeders_tube',\n",
    "       'numfeeders_other', 'population_atleast',\n",
    "       'count_area_size_sq_m_atleast'], axis = 1)\n",
    "sites_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ea291",
   "metadata": {},
   "source": [
    "The joined dataframe `join_df` combines `total_df` and `sites_df` using an `INNER JOIN` on `loc_id`, which provides us with information about the environment in which the observation entry took place. By doing this, we lose some data entries because their location is not described in `sites_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96659136",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = total_df.set_index('loc_id').join(sites_df.set_index('loc_id'), on = \"loc_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a23ce54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>housing_density</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.252988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.252988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.205179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.311377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   housing_density     count\n",
       "0              1.0  0.288845\n",
       "1              2.0  0.252988\n",
       "2              3.0  0.252988\n",
       "3              4.0  0.205179\n",
       "4              1.0  0.311377"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates a list of all subnational1_code in the dataset \n",
    "regions_list = total_df['subnational1_code'].unique()\n",
    "\n",
    "plot_df = pd.DataFrame(columns = ['housing_density', 'count'])\n",
    "\n",
    "def plot_species_counts(state_name):\n",
    "    df = join_df[join_df['subnational1_code'] == state_name]\n",
    "    \n",
    "    x = ['Rural', 'Subrural', 'Suburban', 'Urban']\n",
    "    counts_list = []\n",
    "    \n",
    "    rural_df = df[df[\"housing_density\"] == 1.0]\n",
    "    subrural_df = df[df[\"housing_density\"] == 2.0]\n",
    "    suburban_df = df[df[\"housing_density\"] == 3.0]\n",
    "    urban_df = df[df[\"housing_density\"] == 4.0]\n",
    "    \n",
    "    if len(rural_df) > 0 and len(subrural_df) > 0 and len(suburban_df) > 0 and len(urban_df) > 0:\n",
    "        rural_count = len(rural_df['species_code'].unique())\n",
    "        subrural_count = len(subrural_df['species_code'].unique())\n",
    "        suburban_count = len(suburban_df['species_code'].unique())\n",
    "        urban_count = len(urban_df['species_code'].unique())\n",
    "\n",
    "        counts_sum = rural_count + subrural_count + suburban_count + urban_count\n",
    "\n",
    "        counts_list.append(rural_count / counts_sum)\n",
    "        counts_list.append(subrural_count / counts_sum)\n",
    "        counts_list.append(suburban_count / counts_sum)\n",
    "        counts_list.append(urban_count / counts_sum)\n",
    "\n",
    "        plot_df.loc[len(plot_df)] = [1.0, rural_count / counts_sum]\n",
    "        plot_df.loc[len(plot_df)] = [2.0, subrural_count / counts_sum]\n",
    "        plot_df.loc[len(plot_df)] = [3.0, suburban_count / counts_sum]\n",
    "        plot_df.loc[len(plot_df)] = [4.0, urban_count / counts_sum]\n",
    "    \n",
    "#         sns.scatterplot(x = x, y = counts_list)\n",
    "\n",
    "for state in regions_list:\n",
    "    plot_species_counts(state)\n",
    "    \n",
    "plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510121e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([plot_df, pd.get_dummies(plot_df['housing_density'], drop_first = True,)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN WHEN READY TO EXPORT\n",
    "# new_df.to_csv('all_housing_density.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93facee",
   "metadata": {},
   "source": [
    "## Creating full zero-filled csv for correlation exploration in final paper\n",
    "\n",
    "We first took the original dataset `PFW_2021_public.csv` and sliced it to only include data from NY in order to cut down our dataset. Then, we used an R function provided by FeederWatch to conduct taxonomic roll-up and zero-filling, two procedures recommended by FeederWatch to limit errors. The R code used to clean is provided [here](https://engagement-center.github.io/Project-FeederWatch-Zerofilling-Taxonomic-Rollup-Public/).\n",
    "\n",
    "1. *Taxonomic roll-up*: A process of combining observations that were recorded under different species codes but would best be treated as the same species. For example, some observers may take note of subspecies, which are then recorded under different codes than the overall species when they should logically be combined.\n",
    "\n",
    "2. *Zero-filling*: adding counts of 0 for all species that were not recorded at an observation, essential for accounting for the fact that observation data is inherently presence-only.\n",
    "\n",
    "The resulting csv is `rolled_up_NY_df.csv`, which is then additionally processed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe817fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in raw provided data, zero-filled already, only data from NY\n",
    "csv = pd.read_csv(\"rolled_up_NY_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dataframe\n",
    "df = pd.DataFrame(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = list(map(str.lower, df.columns))\n",
    "df.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cd549",
   "metadata": {},
   "source": [
    "We decided to drop `latitude` and `longitude`. We also dropped irrelevant columns, such as `ENTRY_TECHNIQUE` (a variable indicating method of site localization), `PROJ_PERIOD_ID` (calendar year of end of FeederWatch season), `sub_id` and `obs_id` (indentifiers for checklist or species respectively), `effort_hrs_atleast` (survey time), and `DATA_ENTRY_METHOD` (web/mobile/paper). Only valid observations were kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "df.drop(['unnamed: 0', '...1', 'latitude', 'longitude', 'entry_technique', 'proj_period_id', 'reviewed', 'sub_id', 'obs_id',\n",
    "        'effort_hrs_atleast', 'data_entry_method'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2460f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping observations that are not valid\n",
    "df = df[df['valid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986f86c",
   "metadata": {},
   "source": [
    "By convention, bird species are stored as 6-letter codes. However, this makes readability and interpretability more difficult later on. To remedy this, we can do an inner join with a taxonomy table provided by FeederWatch so we can add a column with the species full common name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining common names\n",
    "df = pd.DataFrame(pd.read_csv(\"PFW-species-translation-table.csv\"))\n",
    "%sql df << SELECT loc_id, subnational1_code, month, day, year, df.species_code, how_many, valid, day1_am, day1_pm, day2_am, day2_pm, snow_dep_atleast, scientific_name, american_english_name AS species_name FROM df INNER JOIN species_translate_df ON df.species_code = species_translate_df.species_code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a20ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows where snow depth was null\n",
    "df = df.dropna(subset=['snow_dep_atleast'])\n",
    "# creating new category with string corresponding to each value in snow depth (for binning in the line plots)\n",
    "df['snow_category'] = 'No_Snow'\n",
    "df.loc[df['snow_dep_atleast'] == 0.001, 'snow_category'] = 'Light_Snow'\n",
    "df.loc[df['snow_dep_atleast'] == 5.000, 'snow_category'] = '5 to 15 cm'\n",
    "df.loc[df['snow_dep_atleast'] == 15.001, 'snow_category'] = 'Heavy_Snow'\n",
    "snow_dummies = pd.get_dummies(df['snow_category'], drop_first=True)\n",
    "df = pd.concat([df, snow_dummies], axis=1)\n",
    "\n",
    "# dropping rows where housing density is null\n",
    "df = join_df.dropna(subset=['housing_density'])\n",
    "# creating new category with string \n",
    "df['housing_density_bins'] = 'rural'\n",
    "df.loc[df['housing_density'] == 2.0, 'housing_density_bins'] = \"rural/suburban\"\n",
    "df.loc[df['housing_density'] == 3.0, 'housing_density_bins'] = \"suburban\"\n",
    "df.loc[df['housing_density'] == 4.0, 'housing_density_bins'] = \"urban\"\n",
    "density_dummies = pd.get_dummies(df['housing_density_bins'], drop_first=True)\n",
    "df = pd.concat([df, density_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999a09a",
   "metadata": {},
   "source": [
    "We also created a joined dataframe `zero_filled_join_df` that combines `df` and `sites_df` using an `INNER JOIN` on `loc_id`, which provides us with information about the environment in which the observation entry took place. By doing this, we lose about half of our `df` data entries because their location is not described in `sites_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40433273",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql zero_filled_join_df << SELECT month, day, year, df.loc_id, species_code, species_name, how_many, day1_am, day1_pm, day2_am, day2_pm, Light_Snow, Heavy_Snow, No_Snow, proj_period_id, yard_type_pavement, yard_type_garden, yard_type_landsca, yard_type_woods, yard_type_desert,hab_dcid_woods, hab_evgr_woods, hab_mixed_woods, hab_orchard, hab_park, hab_water_fresh, hab_water_salt, hab_residential,hab_industrial, hab_agricultural, hab_desert_scrub, hab_young_woods, hab_swamp, hab_marsh, brsh_piles_atleast, water_srcs_atleast, bird_baths_atleast, nearby_feeders, squirrels, cats, dogs, humans, housing_density, rural/suburban, suburban, urban, population_atleast, scientific_name, species_name FROM df INNER JOIN sites_df ON df.loc_id = sites_df.loc_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_filled_join_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN WHEN READY TO EXPORT\n",
    "# zero_filled_join_df.to_csv('zero_filled_join.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9622167",
   "metadata": {},
   "source": [
    "## Creating Accipiter and Junco csvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980ef01",
   "metadata": {},
   "source": [
    "The Accipiter and Junco CSVs are sliced versions of `zero_filled_join_df` that only include entries with genus \"Accipiter\" or \"Junco\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ff560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the scientific name into two columns - the genus and species names\n",
    "zero_filled_join_df[['genus','species']] = zero_filled_join_df['scientific_name'].str.split(expand=True).iloc[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_filled_join_df['genus'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc4a5c",
   "metadata": {},
   "source": [
    "We chose Accipiter and Junco since they are distinct and differing species with a high number of observations in our data. In the final paper, there is a more in-depth explaination for this choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting Accipiter genus data\n",
    "accipiter_df = zero_filled_join_df[zero_filled_join_df['genus'] == 'Accipiter']\n",
    "\n",
    "# selecting Junco genus data\n",
    "junco_df = zero_filled_join_df[zero_filled_join_df['genus'] == 'Junco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only certain columns that we need for futher analysis\n",
    "accipiter_df = accipiter_df[['loc_id', 'genus', 'species', 'how_many', 'species_name', 'housing_density', 'rural/suburban', 'suburban', 'urban', 'population_atleast']]\n",
    "junco_df = junco_df[['loc_id', 'genus', 'species', 'how_many', 'species_name', 'housing_density', 'rural/suburban', 'suburban', 'urban', 'population_atleast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN WHEN READY TO EXPORT\n",
    "# accipiter_df.to_csv('accipiter_df.csv')\n",
    "# junco_df.to_csv('junco_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581bdf6",
   "metadata": {},
   "source": [
    "# STUFF TO MOVE TO OTHER APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONLY RELEVANT FOR REDPOLLS, BUT NOTE THAT THE ABOVE CSVS DO \n",
    "## USE DUMMY VARIABLES FOR SNOW EVEN THOUGH WE DON'T REALLY USE THEM LATER \n",
    "\n",
    "# dropping rows where snow depth was null\n",
    "snow_df = species_limited_df.dropna(subset=['snow_dep_atleast'])\n",
    "\n",
    "# creating new category with string corresponding to each value in snow depth (for binning in the line plots)\n",
    "snow_df['snow_category'] = 'No Snow'\n",
    "snow_df.loc[snow_df['snow_dep_atleast'] == 0.001, 'snow_category'] = '< 5 cm'\n",
    "snow_df.loc[snow_df['snow_dep_atleast'] == 5.000, 'snow_category'] = '5 to 15 cm'\n",
    "snow_df.loc[snow_df['snow_dep_atleast'] == 15.001, 'snow_category'] = '> 15 cm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdf1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS USED TO MAKE THE BLUEJAY_DF.CSV, ONLY USED IN THE OTHER APPENDIX FOR MARY'S REGRESSIONS\n",
    "\n",
    "# making a list of the 10 most frequently observed species\n",
    "frequent_species = df['species_code'].value_counts()[:1].index\n",
    "\n",
    "# creating new dataframe limited to just the most frequent species observations\n",
    "species_limited_df = df[df['species_code'].isin(frequent_species)]\n",
    "\n",
    "species_limited_df.to_csv(\"bluejay_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
