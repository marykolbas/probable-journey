{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad36255",
   "metadata": {},
   "source": [
    "## Data Cleaning (Appendix, Draft) [mary, zero filled accipiter and junco, originaldataset/sites, delete redpolls and snow dummy stuff]\n",
    "Project FeederWatch is a citizen-science-based data source supported by the Cornell Lab of Ornithology, which collects observations of bird species at backyard feeders and habitats all over the world in an annual November-April survey.\n",
    "\n",
    "Our raw file comes from the [Project FeederWatch](https://feederwatch.org/explore/raw-dataset-requests/) 2021 New York checklist data and site description data. This file is extremely large and has sightings from about November 2020 to April 2021. After downloading the file, we used an R function provided by FeederWatch to conduct taxonomic roll-up and zero-filling, two procedures recommended by FeederWatch to limit errors. The R code used to clean is provided [here](https://engagement-center.github.io/Project-FeederWatch-Zerofilling-Taxonomic-Rollup-Public/).\n",
    "\n",
    "1. *Taxonomic roll-up*: A process of combining observations that were recorded under different species codes but would best be treated as the same species. For example, some observers may take note of subspecies, which are then recorded under different codes than the overall species when they should logically be combined.\n",
    "\n",
    "2. *Zero-filling*: adding counts of 0 for all species that were not recorded at an observation, essential for accounting for the fact that observation data is inherently presence-only.\n",
    "\n",
    "We decided to focus our research on only sighting in New York State, which is still a rather large subset of the data. As of right now, we have decided to drop `latitude` and `longitude`. We also dropped irrelevant columns, such as `ENTRY_TECHNIQUE` (a variable indicating method of site localization), `PROJ_PERIOD_ID` (calendar year of end of FeederWatch season), `sub_id` and `obs_id` (indentifiers for checklist or species respectively), `effort_hrs_atleast` (survey time), and `DATA_ENTRY_METHOD` (web/mobile/paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ac3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ad336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sql\n",
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "%sql duckdb:///:memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523607c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in raw provided data\n",
    "csv = pd.read_csv(\"rolled_up_NY_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f5b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dataframe\n",
    "df = pd.DataFrame(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f433a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = list(map(str.lower, df.columns))\n",
    "df.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "df.drop(['unnamed: 0', '...1', 'latitude', 'longitude', 'entry_technique', 'proj_period_id', 'reviewed', 'sub_id', 'obs_id',\n",
    "        'effort_hrs_atleast', 'data_entry_method'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f83ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping observations that are not valid\n",
    "df = df[df['valid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bd052",
   "metadata": {},
   "source": [
    "We also created new dataframes to only specify the top species for our data exploration.\n",
    "\n",
    "`species_limited_df` is a slice of `df` holding the data only for the top (most frequent) species.\n",
    "\n",
    "In `snow_df`, we have created bins (`snow_category`) to categorize the snowfall for all sightings where snow depth is not null.\n",
    "\n",
    "The `species_limited_time` dataframe is a manipulated version of `species_limited_df` with a datetime object, allowing for time series calculations and comparisons.\n",
    "\n",
    "The `df_timeofday` dataframe narrows the data down to only entries where one of the four sighting periods of the two-day observation period occur. In otherwords, only the entries such that `day1_am` + `day1_pm` + `day2_am` + `day2_pm` = 1, and also limits to the top 25 species. This allows us to plot differences in observation counts based on time of day (morning vs afternoon) in our exploratory section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of the 10 most frequently observed species\n",
    "frequent_species = df['species_code'].value_counts()[:1].index\n",
    "\n",
    "# creating new dataframe limited to just the most frequent species observations\n",
    "species_limited_df = df[df['species_code'].isin(frequent_species)]\n",
    "\n",
    "species_limited_df.to_csv(\"bluejay_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22383069",
   "metadata": {},
   "source": [
    "By convention, bird species are stored as 6-letter codes. However, this makes readability and interpretability more difficult later on. To remedy this, we can do an inner join with a taxonomy table provided by FeederWatch so we can add a column with the species full common name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining common names\n",
    "species_translate_df = pd.DataFrame(pd.read_csv(\"PFW-species-translation-table.csv\"))\n",
    "%sql species_limited_df << SELECT loc_id, subnational1_code, month, day, year, species_limited_df.species_code, how_many, valid, day1_am, day1_pm, day2_am, day2_pm, snow_dep_atleast, american_english_name AS species_name FROM species_limited_df INNER JOIN species_translate_df ON species_limited_df.species_code = species_translate_df.species_code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f09825",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_limited_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows where snow depth was null\n",
    "snow_df = species_limited_df.dropna(subset=['snow_dep_atleast'])\n",
    "\n",
    "# creating new category with string corresponding to each value in snow depth (for binning in the line plots)\n",
    "snow_df['snow_category'] = 'No Snow'\n",
    "snow_df.loc[snow_df['snow_dep_atleast'] == 0.001, 'snow_category'] = '< 5 cm'\n",
    "snow_df.loc[snow_df['snow_dep_atleast'] == 5.000, 'snow_category'] = '5 to 15 cm'\n",
    "snow_df.loc[snow_df['snow_dep_atleast'] == 15.001, 'snow_category'] = '> 15 cm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_limited_time = species_limited_df\n",
    "\n",
    "# create a column that puts month, date, year in one string\n",
    "species_limited_time['datestring'] = species_limited_time['month'].astype(str) + \"/\" + species_limited_time['day'].astype(str) + \"/\" + species_limited_time['year'].astype(str)\n",
    "\n",
    "#turn datestring into datetime and drop datestring\n",
    "species_limited_time['date_time'] = pd.to_datetime(species_limited_time['datestring'], format='%m/%d/%Y')\n",
    "species_limited_time.drop(columns=\"datestring\")\n",
    "\n",
    "#grouped by date_time (datetime object)\n",
    "species_limited_time = species_limited_time.groupby([\"species_name\", \"date_time\"]).mean()[['how_many','snow_dep_atleast']]\n",
    "species_limited_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6baee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicating df [MARY DELETES THIS LATER]\n",
    "df_timeofday = df\n",
    "\n",
    "#only using entries with unique time sightings (they only recorded sightings at one of the 4 time periods as opposed\n",
    "#to multiple sightings aggregated)\n",
    "df_timeofday = df_timeofday[df_timeofday['day1_am'] + df_timeofday['day1_pm'] + df_timeofday['day2_am'] + df_timeofday['day2_pm'] == 1]\n",
    "\n",
    "# creating new category with string corresponding to unique sighting day1_am, day1_pm, day2_am, day2_pm\n",
    "df_timeofday['unique_time'] = 'day1_am'\n",
    "df_timeofday.loc[df_timeofday['day1_pm'] == 1, 'unique_time'] = 'day1_pm'\n",
    "df_timeofday.loc[df_timeofday['day2_am'] == 1, 'unique_time'] = 'day2_am'\n",
    "df_timeofday.loc[df_timeofday['day2_pm'] == 1, 'unique_time'] = 'day2_pm'\n",
    "\n",
    "#limit to only top species \n",
    "df_timeofday_specieslimited = df_timeofday[df_timeofday['species_code'].isin(frequent_species)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f38077",
   "metadata": {},
   "source": [
    "We also created a joined dataframe `join_df` that combines `species_limited_df` and `sites_df` using an `INNER JOIN` on `loc_id`, which provides us with information about the environment in which the observation entry took place. By doing this, we lose about half of our `species_limited_df` data entries because their location is not described in `sites_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdefad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in raw provided data\n",
    "csv_sites = pd.read_csv(\"PFW_count_site_data_public_2021.csv\")\n",
    "\n",
    "#creating dataframe\n",
    "sites_df = pd.DataFrame(csv_sites)\n",
    "\n",
    "# keeping only the columns that will be involved in analysis\n",
    "sites_df = sites_df[['loc_id', 'proj_period_id', 'yard_type_pavement', 'yard_type_garden', 'yard_type_landsca', 'yard_type_woods', \n",
    "'yard_type_desert','hab_dcid_woods', 'hab_evgr_woods', 'hab_mixed_woods', 'hab_orchard', 'hab_park', 'hab_water_fresh', \n",
    "'hab_water_salt', 'hab_residential','hab_industrial', 'hab_agricultural', 'hab_desert_scrub', 'hab_young_woods', 'hab_swamp', \n",
    "'hab_marsh', 'brsh_piles_atleast', 'water_srcs_atleast', 'bird_baths_atleast', 'nearby_feeders', 'squirrels', 'cats', 'dogs', 'humans',\n",
    "'housing_density', 'population_atleast']]\n",
    "\n",
    "sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join species_limited_df (sightings of top species) with sites_df (location details)\n",
    "#%sql join_df << SELECT species_limited_df.loc_id, subnational1_code, species_code, how_many, snow_dep_atleast, species_name, date_time, proj_period_id, yard_type_pavement, yard_type_garden, yard_type_landsca, yard_type_woods, yard_type_desert,hab_dcid_woods, hab_evgr_woods, hab_mixed_woods, hab_orchard, hab_park, hab_water_fresh, hab_water_salt, hab_residential,hab_industrial, hab_agricultural, hab_desert_scrub, hab_young_woods, hab_swamp, hab_marsh, brsh_piles_atleast, water_srcs_atleast, bird_baths_atleast, nearby_feeders, squirrels, cats, dogs, humans, housing_density, population_atleast, FROM species_limited_df INNER JOIN sites_df ON species_limited_df.loc_id = sites_df.loc_id;\n",
    "\n",
    "#changed join_df to have these columns for the hypothesis testing section\n",
    "%sql join_df << SELECT month, day, year, species_limited_df.loc_id, species_code, species_name, how_many, date_time, day1_am, day1_pm, day2_am, day2_pm, Light_Snow, Heavy_Snow, No_Snow, proj_period_id, yard_type_pavement, yard_type_garden, yard_type_landsca, yard_type_woods, yard_type_desert,hab_dcid_woods, hab_evgr_woods, hab_mixed_woods, hab_orchard, hab_park, hab_water_fresh, hab_water_salt, hab_residential,hab_industrial, hab_agricultural, hab_desert_scrub, hab_young_woods, hab_swamp, hab_marsh, brsh_piles_atleast, water_srcs_atleast, bird_baths_atleast, nearby_feeders, squirrels, cats, dogs, humans, housing_density, population_atleast, FROM species_limited_df INNER JOIN sites_df ON species_limited_df.loc_id = sites_df.loc_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
