{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2346fb9",
   "metadata": {},
   "source": [
    "## Data Cleaning (Appendix, Draft)\n",
    "\n",
    "Project FeederWatch is a citizen-science-based data source supported by the Cornell Lab of Ornithology, which collects observations of bird species at backyard feeders and habitats all over the world in an annual November-April survey.\n",
    "\n",
    "Our raw file comes from the [Project FeederWatch](https://feederwatch.org/explore/raw-dataset-requests/) 2021 New York checklist data and site description data. This file is extremely large and has sightings from about November 2020 to April 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68269300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688d363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sql\n",
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "%sql duckdb:///:memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69611532",
   "metadata": {},
   "source": [
    "## Joining Original Dataset (not zero-filled) with Sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f24ce9",
   "metadata": {},
   "source": [
    "Our first dataset joins the original dataset of 2021 Season Feederwatch Observations with the sites dataset with location information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e207df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in original dataset from Feederwatch for observations\n",
    "total_df = pd.read_csv(\"PFW_2021_public.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ae56c",
   "metadata": {},
   "source": [
    "We chose to drop several columns in the observations dataset (`total_df`) such that only `loc_id`, `subnational1_code`, and `species_code` are present. We also chose to eliminate most columns from the sites dataset (`sites_df`) so that only `loc_id` and `housing_density` remain, allowing for an efficient data join. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e95db9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>subnational1_code</th>\n",
       "      <th>species_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L12782033</td>\n",
       "      <td>CA-ON</td>\n",
       "      <td>amtspa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L12782033</td>\n",
       "      <td>CA-ON</td>\n",
       "      <td>blujay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L12782033</td>\n",
       "      <td>CA-ON</td>\n",
       "      <td>bkcchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L12755941</td>\n",
       "      <td>CA-SK</td>\n",
       "      <td>dowwoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L12755941</td>\n",
       "      <td>CA-SK</td>\n",
       "      <td>whbnut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loc_id subnational1_code species_code\n",
       "0  L12782033             CA-ON       amtspa\n",
       "1  L12782033             CA-ON       blujay\n",
       "2  L12782033             CA-ON       bkcchi\n",
       "3  L12755941             CA-SK       dowwoo\n",
       "4  L12755941             CA-SK       whbnut"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.columns\n",
    "total_df = total_df.drop(['latitude', 'longitude',\n",
    "       'entry_technique', 'sub_id', 'obs_id', 'Month', 'Day', 'Year',\n",
    "       'PROJ_PERIOD_ID', 'how_many', 'valid', 'reviewed',\n",
    "       'day1_am', 'day1_pm', 'day2_am', 'day2_pm', 'effort_hrs_atleast',\n",
    "       'snow_dep_atleast', 'Data_Entry_Method'], axis = 1)\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2696b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>housing_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L100016</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    loc_id  housing_density\n",
       "0  L100016              2.0\n",
       "1  L100016              2.0\n",
       "2  L100016              2.0\n",
       "3  L100016              2.0\n",
       "4  L100016              2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in original dataset from Feederwatch for locations\n",
    "sites_df_all = pd.read_csv(\"PFW_count_site_data_public_2021.csv\")\n",
    "\n",
    "sites_df = sites_df_all.drop(['proj_period_id', 'yard_type_pavement', 'yard_type_garden',\n",
    "       'yard_type_landsca', 'yard_type_woods', 'yard_type_desert',\n",
    "       'hab_dcid_woods', 'hab_evgr_woods', 'hab_mixed_woods', 'hab_orchard',\n",
    "       'hab_park', 'hab_water_fresh', 'hab_water_salt', 'hab_residential',\n",
    "       'hab_industrial', 'hab_agricultural', 'hab_desert_scrub',\n",
    "       'hab_young_woods', 'hab_swamp', 'hab_marsh', 'evgr_trees_atleast',\n",
    "       'evgr_shrbs_atleast', 'dcid_trees_atleast', 'dcid_shrbs_atleast',\n",
    "       'fru_trees_atleast', 'cacti_atleast', 'brsh_piles_atleast',\n",
    "       'water_srcs_atleast', 'bird_baths_atleast', 'nearby_feeders',\n",
    "       'squirrels', 'cats', 'dogs', 'humans',\n",
    "       'fed_yr_round', 'fed_in_jan', 'fed_in_feb', 'fed_in_mar', 'fed_in_apr',\n",
    "       'fed_in_may', 'fed_in_jun', 'fed_in_jul', 'fed_in_aug', 'fed_in_sep',\n",
    "       'fed_in_oct', 'fed_in_nov', 'fed_in_dec', 'numfeeders_suet',\n",
    "       'numfeeders_ground', 'numfeeders_hanging', 'numfeeders_platfrm',\n",
    "       'numfeeders_humming', 'numfeeders_water', 'numfeeders_thistle',\n",
    "       'numfeeders_fruit', 'numfeeders_hopper', 'numfeeders_tube',\n",
    "       'numfeeders_other', 'population_atleast',\n",
    "       'count_area_size_sq_m_atleast'], axis = 1)\n",
    "sites_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ea291",
   "metadata": {},
   "source": [
    "The joined dataframe `join_df` combines `total_df` and `sites_df` using an `INNER JOIN` on `loc_id`, which provides us with information about the environment in which the observation entry took place. By doing this, we lose some data entries because their location is not described in `sites_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96659136",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = total_df.set_index('loc_id').join(sites_df.set_index('loc_id'), on = \"loc_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23ce54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>housing_density</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.252988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.252988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.205179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.311377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   housing_density     count\n",
       "0              1.0  0.288845\n",
       "1              2.0  0.252988\n",
       "2              3.0  0.252988\n",
       "3              4.0  0.205179\n",
       "4              1.0  0.311377"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates a list of all subnational1_code in the dataset \n",
    "regions_list = total_df['subnational1_code'].unique()\n",
    "\n",
    "plot_df = pd.DataFrame(columns = ['housing_density', 'count'])\n",
    "\n",
    "def plot_species_counts(state_name):\n",
    "    df = join_df[join_df['subnational1_code'] == state_name]\n",
    "    \n",
    "    x = ['Rural', 'Subrural', 'Suburban', 'Urban']\n",
    "    counts_list = []\n",
    "    \n",
    "    rural_df = df[df[\"housing_density\"] == 1.0]\n",
    "    subrural_df = df[df[\"housing_density\"] == 2.0]\n",
    "    suburban_df = df[df[\"housing_density\"] == 3.0]\n",
    "    urban_df = df[df[\"housing_density\"] == 4.0]\n",
    "    \n",
    "    if len(rural_df) > 0 and len(subrural_df) > 0 and len(suburban_df) > 0 and len(urban_df) > 0:\n",
    "        rural_count = len(rural_df['species_code'].unique())\n",
    "        subrural_count = len(subrural_df['species_code'].unique())\n",
    "        suburban_count = len(suburban_df['species_code'].unique())\n",
    "        urban_count = len(urban_df['species_code'].unique())\n",
    "\n",
    "        counts_sum = rural_count + subrural_count + suburban_count + urban_count\n",
    "\n",
    "        counts_list.append(rural_count / counts_sum)\n",
    "        counts_list.append(subrural_count / counts_sum)\n",
    "        counts_list.append(suburban_count / counts_sum)\n",
    "        counts_list.append(urban_count / counts_sum)\n",
    "\n",
    "        plot_df.loc[len(plot_df)] = [1.0, rural_count / counts_sum]\n",
    "        plot_df.loc[len(plot_df)] = [2.0, subrural_count / counts_sum]\n",
    "        plot_df.loc[len(plot_df)] = [3.0, suburban_count / counts_sum]\n",
    "        plot_df.loc[len(plot_df)] = [4.0, urban_count / counts_sum]\n",
    "    \n",
    "#         sns.scatterplot(x = x, y = counts_list)\n",
    "\n",
    "for state in regions_list:\n",
    "    plot_species_counts(state)\n",
    "    \n",
    "plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510121e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>housing_density</th>\n",
       "      <th>count</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288845</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.252988</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.252988</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.205179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   housing_density     count  2.0  3.0  4.0\n",
       "0              1.0  0.288845    0    0    0\n",
       "1              2.0  0.252988    1    0    0\n",
       "2              3.0  0.252988    0    1    0\n",
       "3              4.0  0.205179    0    0    1\n",
       "4              1.0  0.311377    0    0    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.concat([plot_df, pd.get_dummies(plot_df['housing_density'], drop_first = True,)], axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e4c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN WHEN READY TO EXPORT\n",
    "new_df.to_csv('all_housing_density.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93facee",
   "metadata": {},
   "source": [
    "## Creating full zero-filled csv for correlation exploration in final paper\n",
    "\n",
    "We first took the original dataset `PFW_2021_public.csv` and sliced it to only include data from NY in order to cut down our dataset. Then, we used an R function provided by FeederWatch to conduct taxonomic roll-up and zero-filling, two procedures recommended by FeederWatch to limit errors. The R code used to clean is provided [here](https://engagement-center.github.io/Project-FeederWatch-Zerofilling-Taxonomic-Rollup-Public/).\n",
    "\n",
    "1. *Taxonomic roll-up*: A process of combining observations that were recorded under different species codes but would best be treated as the same species. For example, some observers may take note of subspecies, which are then recorded under different codes than the overall species when they should logically be combined.\n",
    "\n",
    "2. *Zero-filling*: adding counts of 0 for all species that were not recorded at an observation, essential for accounting for the fact that observation data is inherently presence-only.\n",
    "\n",
    "The resulting csv is `rolled_up_NY_df.csv`, which is then additionally processed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda73b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in raw provided data, zero-filled already, only data from NY\n",
    "csv = pd.read_csv(\"rolled_up_NY_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dataframe\n",
    "df = pd.DataFrame(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = list(map(str.lower, df.columns))\n",
    "df.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cd549",
   "metadata": {},
   "source": [
    "We decided to drop `latitude` and `longitude`. We also dropped irrelevant columns, such as `ENTRY_TECHNIQUE` (a variable indicating method of site localization), `PROJ_PERIOD_ID` (calendar year of end of FeederWatch season), `sub_id` and `obs_id` (indentifiers for checklist or species respectively), `effort_hrs_atleast` (survey time), and `DATA_ENTRY_METHOD` (web/mobile/paper). Only valid observations were kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "df.drop(['unnamed: 0', '...1', 'latitude', 'longitude', 'entry_technique', 'proj_period_id', 'reviewed', 'sub_id', 'obs_id',\n",
    "        'effort_hrs_atleast', 'data_entry_method'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2460f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping observations that are not valid\n",
    "df = df[df['valid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986f86c",
   "metadata": {},
   "source": [
    "By convention, bird species are stored as 6-letter codes. However, this makes readability and interpretability more difficult later on. To remedy this, we can do an inner join with a taxonomy table provided by FeederWatch so we can add a column with the species full common name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining common names\n",
    "species_translate_df = pd.DataFrame(pd.read_csv(\"PFW-species-translation-table.csv\"))\n",
    "%sql joined_df << SELECT loc_id, subnational1_code, month, day, year, df.species_code, how_many, valid, day1_am, day1_pm, day2_am, day2_pm, snow_dep_atleast, species_translate_df.scientific_name, species_translate_df.american_english_name AS species_name FROM df INNER JOIN species_translate_df ON df.species_code = species_translate_df.species_code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ba4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a20ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows where snow depth was null\n",
    "joined_df = joined_df.dropna(subset=['snow_dep_atleast'])\n",
    "# creating new category with string corresponding to each value in snow depth (for binning in the line plots)\n",
    "joined_df['snow_category'] = 'No_Snow'\n",
    "joined_df.loc[joined_df['snow_dep_atleast'] == 0.001, 'snow_category'] = 'Light_Snow'\n",
    "joined_df.loc[joined_df['snow_dep_atleast'] == 5.000, 'snow_category'] = '5 to 15 cm'\n",
    "joined_df.loc[joined_df['snow_dep_atleast'] == 15.001, 'snow_category'] = 'Heavy_Snow'\n",
    "snow_dummies = pd.get_dummies(joined_df['snow_category'], drop_first=True)\n",
    "joined_df = pd.concat([joined_df, snow_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fe354",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.to_csv('temporary_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb801d",
   "metadata": {},
   "source": [
    "`temporary_df.csv` is then exported and joined with `sites_df` in SQLiteStudio because this Python SQL join kills the Kernel on our computers. Using the following query: \n",
    "\n",
    "1. **Query to create joined_on_sites table**\n",
    "```\n",
    "SELECT month, day, year, temp_df.loc_id, species_code, species_name, how_many, snow_dep_atleast, Light_Snow, Heavy_Snow, No_Snow, housing_density, population_atleast, scientific_name, species_name FROM temp_df INNER JOIN sites_df ON temp_df.loc_id = sites_df.loc_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846f582",
   "metadata": {},
   "source": [
    "Similarly, we ran into problems creating dummy variables using python on such a large dataframe. Instead, these queries were run in SQLiteStudio:\n",
    "\n",
    "2. **Query to create housing_bins table**\n",
    "\n",
    "```\n",
    "SELECT month, day, year, loc_id, species_code, species_name, how_many, snow_dep_atleast, Light_Snow, Heavy_Snow, No_Snow, housing_density, population_atleast, scientific_name, species_name,\n",
    "    IIF(housing_density = '1', 'rural',\n",
    "        IIF(housing_density = '2', 'rural/suburban',\n",
    "            IIF(housing_density = '3', 'suburban','urban')\n",
    "        )\n",
    "    ) AS housing_density_bins\n",
    "FROM\n",
    "    joined_on_sites\n",
    "```\n",
    "\n",
    "\n",
    "3. **Query to create dummies table**\n",
    "\n",
    "```\n",
    "SELECT month, day, year, loc_id, species_code, species_name, how_many, snow_dep_atleast, Light_Snow, Heavy_Snow, No_Snow, housing_density, population_atleast, scientific_name, species_name, housing_density_bins,\n",
    "    IIF(housing_density_bins = 'rural', 1, 0) AS rural,\n",
    "    IIF(housing_density_bins = 'rural/suburban', 1, 0) AS rural_suburban,\n",
    "    IIF(housing_density_bins = 'suburban', 1, 0) AS suburban,\n",
    "    IIF(housing_density_bins = 'urban', 1, 0) AS urban\n",
    "FROM\n",
    "    housing_bins\n",
    "```\n",
    "\n",
    "4. **Query to create final table with genus names, genus_split** \n",
    "```\n",
    "SELECT month, day, year, loc_id, species_code, species_name, how_many, snow_dep_atleast, Light_Snow, Heavy_Snow, No_Snow, housing_density, population_atleast, scientific_name, species_name, housing_density_bins, rural, rural_suburban, suburban, urban,\n",
    "    SUBSTRING(scientific_name, 1, IIF(INSTR(scientific_name, ' ')-1 > 0, INSTR(scientific_name, ' ')-1, LENGTH(scientific_name))) AS genus\n",
    "FROM dummies\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999a09a",
   "metadata": {},
   "source": [
    "This final joined dataframe with housing density dummy variables and genus names was exported as `zero_filled_join_df.csv`. Unforunately, reading in such dataframe into this Jupyter Notebook causes memory errors on our computers, even when using chunk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9622167",
   "metadata": {},
   "source": [
    "## Creating Accipiter and Junco csvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980ef01",
   "metadata": {},
   "source": [
    "The Accipiter and Junco CSVs are sliced versions of `zero_filled_join_df` that only include entries with genus \"Accipiter\" or \"Junco\" respectively. These similarly gave us trouble in python because `zero_filled_join_df` cannot be read into this notebook. The following queries were run in SQLiteStudio and were exported as `accipiter_df.csv` and `junco_df.csv`:\n",
    "\n",
    "```\n",
    "SELECT month, day, year, loc_id, species_code, species_name, how_many, snow_dep_atleast, Light_Snow, Heavy_Snow, No_Snow, housing_density, population_atleast, scientific_name, species_name, housing_density_bins, rural, rural_suburban, suburban, urban, genus\n",
    "FROM genus_split WHERE genus = 'Accipiter'\n",
    "```\n",
    "\n",
    "```\n",
    "SELECT month, day, year, loc_id, species_code, species_name, how_many, snow_dep_atleast, Light_Snow, Heavy_Snow, No_Snow, housing_density, population_atleast, scientific_name, species_name, housing_density_bins, rural, rural_suburban, suburban, urban, genus\n",
    "FROM genus_split WHERE genus = 'Junco'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffee2bc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 256. KiB for an array with shape (32768,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accipiter_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccipiter_df.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m accipiter_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1255\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1253\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1255\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1172\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\info2950\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1731\u001b[0m, in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 256. KiB for an array with shape (32768,) and data type int64"
     ]
    }
   ],
   "source": [
    "accipiter_df = pd.read_csv('accipiter_df.csv')\n",
    "accipiter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "junco_df = pd.read_csv('junco_df.csv')\n",
    "junco_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
